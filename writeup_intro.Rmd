---
title: "Draftsim analysis"
output: html_notebook
---

Some R code to analyze Draftsim drafting results.

```{r}
require(dplyr)
require(ggplot2)
require(tidyr)
load("C:/Users/Sysadmin/Documents/draftsim/pairs variable M19 part 1 v3.RData")
```

Let me start with describing the data. The exact structure of this data is not that important for the analysis below, but I will be referring to it, so we can as well describe it. The data used for this workbook comes from the analysis of 48137 drafts of M19 dataset that users ran on draftsim web-site (http://draftsim.com) in August 2018. 

I loaded this data in R, and wrote a simple code that went through the final draft piles one by one, calculating the total number of cases that every two cards from the set (i and j) were drafted together (n_ij). This data was stored in a matrix called "pairs". 

Now, to be clear, if in one particular draft we had a card i, and it was drafted together with two copies of card j, I would count this pair twice. This is a very important point for frequent cards (commons): I did not count the number of _piles_ that had both cards i and j in them; I counted the number of _card pairs_ that happened to be cards i and j. This will be important later. 

Also this definition means that n_ii (the number of times card i was drafted with itself) is a thing, as it shows how many times a pile would contain 2 cards that both happened to be card #i. The more often card i was drafted repeatedly, the higher n_ii would be.

To make calculations simpler, I also counted the total number of times each card was drafted at all, and stored it in a variable "freq". Tehnically this can be reconstructed from pairs, but why would I do it if there's an easier way.

For each card, I also calculated the average step within the draft at which it was picked. I called this variable "rank", but actually a more correct name would be something like "pick order", as it's not a rank, and also it is reversed: the higher this value, the later in a draft this card was picked, on average. For technical reasons, for this value I only looked at the first booster out of 3 (at the moment of this writing, we had some issues with storing the exact sequence of card picks for later boosters). Also, for this "rank" value I didn't correct for synergies in any way: that is, I did not try to adjust the value  when the same red card was picked late in a blue draft, but early on in a red draft. I assume that with 50 000 total drafts, these effects will largely average out, although of course there are better ways to look at this problem.

And lastly, to visualize the cards nicely I also had a database that for each card stores some useful information about it, named "db":

```{r}
names(db)
```

The fields that are important here are "id", "name", and casting cost. All names were turned lowercase, with spaces replaced by underscores, and with commas purged. Then I looked at the "cost" field, and split it into separate costs of each mana (fields WUBRG) using the command shown below. This was later used to classify cards into colors and guilds:

```{r}
db <- mutate(db,w = grepl("W",cost)*1,
                u = grepl("U",cost)*1,
                b = grepl("B",cost)*1,
                r = grepl("R",cost)*1,
                g = grepl("G",cost)*1) # Number of symbols for each mana in cost
```

All other fields may be ignored.

So, let's look at the "pairs" matrix. Again, it contains the data about every co-occurrence of every two cards in final draft piles. That is, if out of entire 48137 drafts, cards i and j were drafted together only once, the ij element of this matrix will be equal to 1. If these two cards were co-drafted 100 times, the value will be equal to 100. And for now this matrix is asymmetric, as I only filled it for i<=j. Here's how it looks like right after it was built by the loop running through all draft results:

```{r}
image(pairs,axes=F,col = grey(seq(0, 1, length = 256)))
```
Here's the distribution of the upper part of "pairs" values:

(Tehnically this formula is a bit wrong, as it double-counts cards on the diagonal p_ii, for cards that were drafted in 2 copies, but it's close enough to the truth for a quick and dirty histogram)

```{r}
ggplot() + geom_histogram(aes(x=c(pairs+t(pairs))),bins=200) + theme_bw()
```
Some cards were drafted together thousands of times, but most were only drafted together a few hundred times or so. Here's a zoom-in on the leftmost part of the histogram.

```{r}
temp <- c(pairs+t(pairs))
ggplot() + geom_histogram(aes(x=temp[temp<1000]),bins=200) + theme_bw()
```
Out of curiosity, what's the rarest pair of cards drafted?

```{r}
min(temp)
```
```{r}
ij <- which(pairs == min(temp), arr.ind = TRUE)
ij
```
```{r}
db[ij[1],1]
```
```{r}
db[ij[2],1]
```
A 2-color mythic and a rare of a non-matching color. No wonder.



Well, let's go further. The next  thing we need to do now is to make this matrix symmetric, and normalize it, to move from numbers to probabilities:

    p_ij = n_ij/nPairs = n_ji/(pileSize*(pileSize-1)*nDrafts)

where 'p_ij' is the probability that a randomly chosen pair of cards from a pile is actually a pair containing one card i and one card j.

Another important player here is the probability that a randomly chosen card from your pile is card i. It's given by an even more obvious formula:

    p_i = n_i/(pileSize*nDrafts)

Now, cards i and j are drafted independently from each other, that is, if the player doesn't care about the presence of card i in their hand, while drafting card j, and the other way around, then we'll have 'p_ij = p_i * p_j'

In practice some cards are of course drafted together more often, and so p_ij > p_i * p_j, while some cards are drafted together more rarely. We can quantify this preference (or anti-prefereence) as a coefficient a, with  the following formula:

    a_ij = p_ij/(p_i * p_j)
    
or

    a_ij = n_ij/(pileSize*(pileSize-1)*nDrafts)/(n_i*n_j/(pileSize*nDrafts)^2) = 
    = n_ij/(n_i*n_j) * pileSize/(pileSize-1) * nDrafts
    
Or, if you don't want to get that mathy, you can just calculate empyrical probabilities, and roll with them:

    p_i = n_i/sum(n_i)
    p_j = n_j/sum(n_i)
    p_ij = n_ij/sum(n_ij)
    a_ij = p_ij/(p_i * p_j)
    
Which is exactly what I did, as actual data was messier than the neat assumptions I wrote above; for example, some boosters had foil lands in them, which occupied a spot of a normal card, thus reducing the effective size of a meaningful pile by one. Things like that don't happen too often, but it's just easier to go with empirical values, to have a self-coorrecting code (although admittedly it may sometimes mask bugs).

```{r}
nCards <- nrow(db)                 # Repeat, in case the data was reloaded
nTotalPicks <- sum(freq)
nTotalPairs <- sum(pairs)
p <- pairs                         # Make a copy
for(jCard in 1:nCards) {
    for(iCard in 1:jCard) {
      p[iCard,jCard] <- p[iCard,jCard]/nTotalPairs/(freq[iCard]*freq[jCard])*nTotalPicks^2
      p[jCard,iCard] <- p[iCard,jCard]
    }
}

```

Now if we look at this matrix, it will look different:

```{r}
image(p,axes=F,col = grey(seq(0, 1, length = 256)))
```

What is the highest synergy, if calculated that way? 

```{r}
max(p)
```

And which pair of cards turned to be the most "synergistic"?

```{r}
ij <- which(p == max(p), arr.ind = TRUE)
db[ij[1],1]
```
```{r}
db[ij[2],1]
```

Curiously, it's a synergy of Elvish Clancaller with itself. The card is not surprizing, it's an elf lord after all, but it is curious that with this calculation, rares clearly have an edge over commons and non-commons, as while nobody would ever pass over two cool synergistic rares, drafted in two consecutive boosters, people would sometimes pass over a synergistic common or uncommon.

In case you were wondering, the second most synergistic pair in this set is a combo of "blanchwood_armor" with "druid_of_horns" (a cool enchantment and a cool creature that benefits from enchantments).

Now, time to go from these relative mutual probabilities to distances, to make a nice plot. Mutual probabilities are high if two cards are often drafted together, but we want a distance-like value that would be low if two cards are often drafted together. There are many formulas that one can use here, but we'll go with a simplest one:

```{r}
dist <- (1-0.99*p/max(p))
```

I use 0.99 to keep the closest cards still separated a bit, just in case.

Now, there is a whole set of methods that are willing to take a matrix of pairwise distances and try to find an arrangement of points in a space of some dimension that would approximate these pairwise distances as well as possible. The idea behind the methods is simple: throw some points out there, and move them around, until some measure of alingment is maximised. Say, one can try to maxime Pearson correlation between pairwise distances in this new low-dimensional space, and "input pairwise distances".

The simplest approach from this family is called MDS, or multidimensional scaling. Let's apply it to our data:

```{r}
ndim <- 2 
fit <- cmdscale(dist,eig=TRUE, k=ndim)
scale <- data.frame(x=fit$points[,1],y=fit$points[,2])
```

Now we have a dataset with x and y points for each card, and the distribution of this points approximates target pairwise distances as well as possible (in some sense, at least). The points are nice, but how to interpret them?

```{r}
ggplot(scale,aes(x,y)) + theme_bw() + geom_point() + coord_fixed()
```
We'll have to draw from card database "db", and combine this dataset with newly calculated x and y points. If you remember, in my "db" set I calculated mana costs for each mana color, and stored them in fields names WUBRG respectively. I'll now need to somehow transform these WUBRG fields into one field with "color" values. My solution (below) is a bit weird, but hey, it works.

```{r}
scale <- mutate(scale,id=1:nrow(scale))       # Add a column of ids (as the sequence of cards matched ids from db)
scale <- inner_join(scale,db,by="id")         # Bring all fields from db to scale, useful and useless alike
scale <- mutate(scale,color_n=0)              # What is the color of this card, if coded as a number?
scale <- mutate(scale,color_n=w*1+u*2+b*3+r*4+g*5) 
scale <- mutate(scale,color_n=color_n*((w+u+b+r+g)==1)+6*((w+u+b+r+g)>1))

# Now we need to translate these numbers into factors. I'll use an SQL-style approach; maybe there's a simpler way
colorList <- data.frame(color_n=c(0,1,2,3,4,5,6),
                        color=c("0","W","U","B","R","G","M"))
colorList$color = factor(colorList$color,levels=c("0","W","U","B","R","G","M"),ordered=T) # Don't order alphabetically!
scale <- inner_join(scale,colorList,by="color_n") # Done
```

Time to plot same picture as above, but with proper colors!

```{r}
myColors <- c("gray","tan2","blue","black","red","green","purple")
ggplot(scale,aes(x,y,color=color)) + theme_bw() + geom_point() + coord_fixed() +
  scale_color_manual(values=myColors) + xlab('') + ylab('')
```

That's a nice and interesting plot! Please read our [Post at Draftsim blog](https://draftsim.com/blog/draft-data-analysis/) for a full detailed description of how to read it, and how to interpret it.

To make a plot with labels, I used a package ggrepel. This plot takes a while to generate, and it looks horrible in a workbook, but IRL one can save it at 2000x2000 px, as I did for the blog post, which made it quite readable.

```{r}
require(ggrepel)
ggplot(scale) + theme_bw() + geom_point(aes(x,y,color=color)) +
  scale_color_manual(values=c("gray","tan2","blue","black","red","green","purple")) +
  geom_text_repel(aes(x,y,label=name),size=2,box.padding = 0.01, point.padding = 0.01) +
  xlab('') + ylab('') 
```

Now, let's look at the pick order. We'll need to attach the "rank" variable to our data frame, and then we can visualize it:

```{r}
scale <- mutate(scale,rank=rank)

ggplot(scale,aes(color,rank(rank),color=color)) + theme_bw() + geom_point() +
  geom_text(aes(label=name),color='black',size=2,hjust="left",position = position_nudge(x = 0.05)) +
  scale_color_manual(values=c("gray","tan2","blue","black","red","green","purple"))
```
Again, this plot doesn't necessarily look good at this resolution, but it would look better if scaled properly.

Finally, what about 3d plots for the paper? To get a 3d plot, I would reset ndim to 3:
```{r}
ndim <- 3
```

then rerun the mds analysis (but now in 3D rather than 2D), and visualize it with RGL package:

```{r}
require(rgl)
require(car)
require(magick)

plot3d(scale$x,scale$y,scale$z,surface=F,col=myColors[scale$color],
       xlab="", ylab="", zlab="", size=4, alpha=0.8)
# movie3d(spin3d(axis = c(0,0,1), rpm = 2), duration=10, type="gif", dir=myFolder)
```

I won't be running this code here, but you can see a beautiful rotating 3D plot [in the original article](https://draftsim.com/blog/draft-data-analysis/) 
